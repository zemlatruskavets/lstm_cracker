{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "    ██████╗ ██╗██╗     ███████╗████████╗███╗   ███╗     ██████╗██████╗  █████╗  ██████╗██╗  ██╗███████╗██████╗ \n",
    "    ██╔══██╗██║██║     ██╔════╝╚══██╔══╝████╗ ████║    ██╔════╝██╔══██╗██╔══██╗██╔════╝██║ ██╔╝██╔════╝██╔══██╗\n",
    "    ██████╔╝██║██║     ███████╗   ██║   ██╔████╔██║    ██║     ██████╔╝███████║██║     █████╔╝ █████╗  ██████╔╝\n",
    "    ██╔══██╗██║██║     ╚════██║   ██║   ██║╚██╔╝██║    ██║     ██╔══██╗██╔══██║██║     ██╔═██╗ ██╔══╝  ██╔══██╗\n",
    "    ██████╔╝██║███████╗███████║   ██║   ██║ ╚═╝ ██║    ╚██████╗██║  ██║██║  ██║╚██████╗██║  ██╗███████╗██║  ██║\n",
    "    ╚═════╝ ╚═╝╚══════╝╚══════╝   ╚═╝   ╚═╝     ╚═╝     ╚═════╝╚═╝  ╚═╝╚═╝  ╚═╝ ╚═════╝╚═╝  ╚═╝╚══════╝╚═╝  ╚═╝\n",
    "                                                                                                           \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](http://www.treasurenet.com/forums/attachment.php?attachmentid=173574&amp;d=1332348453)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "This module trains a bidirectional long short-term memory (LSTM) network on a dataset consisting of cleartext passwords. The trained network is then used to predict the most likely alterations and/or additions to a given sequence.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### Assumptions\n",
    "The dataset is assumed to contain no information other than the cleartext passwords.\n",
    "\n",
    "The network parameters (*e.g.*, number of hidden units, embedding layer, *etc.*) are defined in the configuration file (`program/config.yml`).\n",
    "\n",
    "\n",
    "### Code steps\n",
    "This is the basic flow of the code:\n",
    "\n",
    "1. read in data\n",
    "   * clean up data (duplicates, NaN, etc)  \n",
    "2. get data characteristics\n",
    "   * determine number of characters  \n",
    "   * determine/define longest sequence length  \n",
    "3. generator\n",
    "   * tokenization  \n",
    "   * sliding windows  \n",
    "4. training\n",
    "5. sequence\n",
    "   * for i in sequence, predict most likely candidates in each position  \n",
    "   * calculate most likely shared candidates  \n",
    "   * calculate probabilities of overall adjusted sequences  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Initial Definitions\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the libraries used in this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import boto3\n",
    "import time\n",
    "import os\n",
    "import keras\n",
    "\n",
    "# sagemaker libraries\n",
    "import sagemaker\n",
    "from sagemaker.tuner              import IntegerParameter, CategoricalParameter, ContinuousParameter, HyperparameterTuner\n",
    "from sagemaker.tensorflow         import TensorFlow\n",
    "from sagemaker.tensorflow.serving import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define all of the variables used in the notebook here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the S3 bucket parameters\n",
    "bucket = 'blstm-cracker'\n",
    "prefix = 'test-run'\n",
    "\n",
    "# get the session and IAM role information\n",
    "sess = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role\n",
    "\n",
    "# location and name of the program containing all of the code\n",
    "program_name = 'program.py'\n",
    "program_path = 'program'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the variables related to the model artifacts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# locations in which to store model artifacts\n",
    "intermediate_location = 's3://{}/{}/intermediate'.format(bucket, prefix)\n",
    "output_location       = 's3://{}/{}/output/'.format(bucket, prefix)\n",
    "\n",
    "# specify the location in S3 containing the dataset\n",
    "data_path     = 'data/dump.csv'\n",
    "data_name     = 'train.csv'\n",
    "key           = os.path.join(prefix, 'train', data_name)\n",
    "s3_train_data = 's3://{}/{}'.format(bucket, key)\n",
    "\n",
    "# define the intermediate path where the model artifacts will be stored\n",
    "inter      = os.path.join(prefix, 'intermediate')\n",
    "inter_path = 's3://{}/{}'.format(bucket, inter)\n",
    "\n",
    "# define the output path\n",
    "out         = os.path.join(prefix, 'output')\n",
    "output_path = 's3://{}/{}'.format(bucket, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the variables related to the endpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to model artifacts\n",
    "model_artifacts = 's3://{}/{}/output/cloud-test/output/model.tar.gz'.format(bucket, prefix)\n",
    "\n",
    "# include the date in the endpoint name\n",
    "endpoint_name = 'keras-tf-fmnist-'+time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the SageMaker session and IAM role information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/aws/sagemaker-python-sdk/issues/911\n",
    "# https://towardsdatascience.com/building-fully-custom-machine-learning-models-on-aws-sagemaker-a-practical-guide-c30df3895ef7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Data\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next section uploads the dataset to the S3 bucket:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload the training data to S3\n",
    "boto3.resource('s3').Bucket(bucket).Object(key).put(Body=open(data_path, 'rb'))\n",
    "print('Uploading data to: {}'.format(s3_train_data))\n",
    "\n",
    "# configure SageMaker input channel\n",
    "input_data = {\n",
    "    'training': sagemaker.session.s3_input(s3_train_data, distribution='FullyReplicated', content_type='text/csv')\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Model\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the hyperparameters for the training job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters={'epochs':       5, \n",
    "                 'batch_size':   128,\n",
    "                 'hidden_units': 100,\n",
    "                 'training':     s3_train_data}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the TensorFlow estimator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = TensorFlow(entry_point          = program_name, \n",
    "                       role                 = role,\n",
    "                       source_dir           = program_path,\n",
    "                       model_dir            = intermediate_location,\n",
    "                       output_path          = output_location,\n",
    "                       code_location        = intermediate_location,\n",
    "                       train_instance_count = 1, \n",
    "                       train_instance_type  = 'local',\n",
    "                       framework_version    = '1.12', \n",
    "                       py_version           = 'py3',\n",
    "                       script_mode          = True,\n",
    "                       hyperparameters      = hyperparameters\n",
    "                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the model using the hyperparameters and estimator defined above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.fit(inputs=input_data, job_name='AJT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameter_ranges = {\n",
    "    'epochs':        IntegerParameter(20, 100),\n",
    "    'learning-rate': ContinuousParameter(0.001, 0.1, scaling_type='Logarithmic'), \n",
    "    'batch-size':    IntegerParameter(32, 1024),\n",
    "    'dense-layer':   IntegerParameter(128, 1024),\n",
    "    'dropout':       ContinuousParameter(0.2, 0.6)\n",
    "}\n",
    "\n",
    "objective_metric_name = 'val_acc'\n",
    "objective_type        = 'Maximize'\n",
    "metric_definitions    = [{'Name': 'val_acc', 'Regex': 'val_acc: ([0-9\\\\.]+)'}]\n",
    "\n",
    "tuner = HyperparameterTuner(tf_estimator,\n",
    "                            objective_metric_name,\n",
    "                            hyperparameter_ranges,\n",
    "                            metric_definitions,\n",
    "                            max_jobs=10,\n",
    "                            max_parallel_jobs=2,\n",
    "                            objective_type=objective_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Endpoint\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the model from its artifacts stored on S3 and use this to deploy an endpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model from its artifacts on S3\n",
    "model = Model(model_data=model_artifacts, role=role)\n",
    "\n",
    "# deploy an endpoint\n",
    "predictor = model.deploy(initial_instance_count=1, \n",
    "                         instance_type='ml.t2.medium',\n",
    "                         endpoint_name=endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete the endpoint after it is no longer needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
