{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "    ██████╗ ██╗██╗     ███████╗████████╗███╗   ███╗     ██████╗██████╗  █████╗  ██████╗██╗  ██╗███████╗██████╗ \n",
    "    ██╔══██╗██║██║     ██╔════╝╚══██╔══╝████╗ ████║    ██╔════╝██╔══██╗██╔══██╗██╔════╝██║ ██╔╝██╔════╝██╔══██╗\n",
    "    ██████╔╝██║██║     ███████╗   ██║   ██╔████╔██║    ██║     ██████╔╝███████║██║     █████╔╝ █████╗  ██████╔╝\n",
    "    ██╔══██╗██║██║     ╚════██║   ██║   ██║╚██╔╝██║    ██║     ██╔══██╗██╔══██║██║     ██╔═██╗ ██╔══╝  ██╔══██╗\n",
    "    ██████╔╝██║███████╗███████║   ██║   ██║ ╚═╝ ██║    ╚██████╗██║  ██║██║  ██║╚██████╗██║  ██╗███████╗██║  ██║\n",
    "    ╚═════╝ ╚═╝╚══════╝╚══════╝   ╚═╝   ╚═╝     ╚═╝     ╚═════╝╚═╝  ╚═╝╚═╝  ╚═╝ ╚═════╝╚═╝  ╚═╝╚══════╝╚═╝  ╚═╝\n",
    "                                                                                                           \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import boto3\n",
    "import csv\n",
    "import time\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import keras\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# sagemaker libraries\n",
    "import sagemaker\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "from sagemaker.tuner      import IntegerParameter, CategoricalParameter, ContinuousParameter, HyperparameterTuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the session information\n",
    "sess = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading data to: s3://blstm-cracker/test-run/train/train.csv\n"
     ]
    }
   ],
   "source": [
    "# define S3 settings\n",
    "bucket = 'blstm-cracker'\n",
    "prefix = 'test-run'\n",
    "\n",
    "# define the path to which the data will be uploaded\n",
    "data_path     = 'lstm_cracker/data/dump.csv'\n",
    "data_name     = 'train.csv'\n",
    "key           = os.path.join(prefix, 'train', data_name)\n",
    "s3_train_data = 's3://{}/{}'.format(bucket, key)\n",
    "\n",
    "# define the output path\n",
    "out         = os.path.join(prefix, 'output')\n",
    "output_path = 's3://{}/{}'.format(bucket, out)\n",
    "\n",
    "# upload the training data to S3\n",
    "print('Uploading data to: {}'.format(s3_train_data))\n",
    "boto3.resource('s3').Bucket(bucket).Object(key).put(Body=open(data_path, 'rb'))\n",
    "\n",
    "# configure SageMaker input channel\n",
    "input_data = {\n",
    "    'training': sagemaker.session.s3_input(s3_train_data, distribution='FullyReplicated', content_type='text/csv')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m# -*- coding: utf-8 -*-\u001b[39;49;00m\r\n",
      "\u001b[33m\"\"\"\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33mThis module trains a bidirectional long short-term memory (LSTM) \u001b[39;49;00m\r\n",
      "\u001b[33mnetwork on a dataset consisting solely of cleartext passwords.\u001b[39;49;00m\r\n",
      "\u001b[33mThe trained network is then used to predict the most likely\u001b[39;49;00m\r\n",
      "\u001b[33malterations and/or additions to a given sequence.\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33mExample\u001b[39;49;00m\r\n",
      "\u001b[33m-------\u001b[39;49;00m\r\n",
      "\u001b[33m    To run the program, include the dataset containing the cleartext \u001b[39;49;00m\r\n",
      "\u001b[33m    passwords as the first argument. The code will handle the rest.\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m    $ python3 program.py\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33mNotes\u001b[39;49;00m\r\n",
      "\u001b[33m-----\u001b[39;49;00m\r\n",
      "\u001b[33m    The dataset is assumed to contain two columns: the usernames and the \u001b[39;49;00m\r\n",
      "\u001b[33m    cleartext passwords.\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m    The network parameters (e.g., number of hidden units, embedding\u001b[39;49;00m\r\n",
      "\u001b[33m    layer, etc.) are defined in the configuration file (config.yml).\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m    This is the basic flow of the code:\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m    1) read in data\u001b[39;49;00m\r\n",
      "\u001b[33m        1) clean up data (duplicates, NaN, etc)\u001b[39;49;00m\r\n",
      "\u001b[33m    2) get data characteristics\u001b[39;49;00m\r\n",
      "\u001b[33m        1) determine number of characters\u001b[39;49;00m\r\n",
      "\u001b[33m        2) determine/define longest sequence length\u001b[39;49;00m\r\n",
      "\u001b[33m    3) generator\u001b[39;49;00m\r\n",
      "\u001b[33m        3.1) tokenization\u001b[39;49;00m\r\n",
      "\u001b[33m        3.2) sliding windows\u001b[39;49;00m\r\n",
      "\u001b[33m    4) training\u001b[39;49;00m\r\n",
      "\u001b[33m    5) sequence\u001b[39;49;00m\r\n",
      "\u001b[33m        5.1) for i in sequence, predict most likely candidates in each position\u001b[39;49;00m\r\n",
      "\u001b[33m        5.2) calculate most likely shared candidates\u001b[39;49;00m\r\n",
      "\u001b[33m        5.3) calculate probabilities of overall adjusted sequences\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33mAttributes\u001b[39;49;00m\r\n",
      "\u001b[33m----------\u001b[39;49;00m\r\n",
      "\u001b[33ms3 : str\u001b[39;49;00m\r\n",
      "\u001b[33m    This variable holds connection information and allows typical file-system \u001b[39;49;00m\r\n",
      "\u001b[33m    style operations to interact with files stored in an S3 bucket.\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33mvariables : dict\u001b[39;49;00m\r\n",
      "\u001b[33m    This dictionary holds the configuration variables defined in config.yml.\u001b[39;49;00m\r\n",
      "\u001b[33m    \u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33mTo do\u001b[39;49;00m\r\n",
      "\u001b[33m-----\u001b[39;49;00m\r\n",
      "\u001b[33m1) Put the parts saving data to S3 into a single function and invoke that\u001b[39;49;00m\r\n",
      "\u001b[33m2) Think through the guessing code.\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m\"\"\"\u001b[39;49;00m\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\u001b[37m################################################################################\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[37m##                              IMPORT MODULES                                ##\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[37m################################################################################\u001b[39;49;00m\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mdateutil.parser\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mdp\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mgc\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mmultiprocessing\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m  \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpandas\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mpd\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpickle\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpsutil\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mrandom\u001b[39;49;00m\r\n",
      "\u001b[37m# import s3fs\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msys\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36muuid\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36myaml\u001b[39;49;00m\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mdatetime\u001b[39;49;00m                     \u001b[34mimport\u001b[39;49;00m datetime, time, timedelta\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mconcurrent.futures\u001b[39;49;00m           \u001b[34mimport\u001b[39;49;00m ProcessPoolExecutor\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mconcurrent.futures\u001b[39;49;00m           \u001b[34mimport\u001b[39;49;00m ThreadPoolExecutor\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mfunctools\u001b[39;49;00m                    \u001b[34mimport\u001b[39;49;00m partial\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mgenerator\u001b[39;49;00m                    \u001b[34mimport\u001b[39;49;00m DataGenerator\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mkeras.callbacks\u001b[39;49;00m              \u001b[34mimport\u001b[39;49;00m ModelCheckpoint, EarlyStopping\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mkeras.layers\u001b[39;49;00m                 \u001b[34mimport\u001b[39;49;00m Embedding, LSTM, Dense, Bidirectional\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mkeras.models\u001b[39;49;00m                 \u001b[34mimport\u001b[39;49;00m Sequential, load_model\r\n",
      "\u001b[37m# from keras.preprocessing.text     import Tokenizer, tokenizer_from_json\u001b[39;49;00m\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mkeras.preprocessing.sequence\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m pad_sequences\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mkeras.utils\u001b[39;49;00m                  \u001b[34mimport\u001b[39;49;00m to_categorical\r\n",
      "\u001b[37m# from pympler.asizeof              import asizeof\u001b[39;49;00m\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msklearn.model_selection\u001b[39;49;00m      \u001b[34mimport\u001b[39;49;00m train_test_split\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mstatistics\u001b[39;49;00m                   \u001b[34mimport\u001b[39;49;00m median\r\n",
      "\u001b[37m# from tqdm                         import tqdm\u001b[39;49;00m\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\u001b[37m# open access to S3 bucket\u001b[39;49;00m\r\n",
      "\u001b[37m# s3 = s3fs.S3FileSystem(s3_additional_kwargs={'ServerSideEncryption': 'AES256'})\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[37m# Import variables from config file\u001b[39;49;00m\r\n",
      "\u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mconfig.yml\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mr\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m config:\r\n",
      "    variables = yaml.load(config, Loader=yaml.FullLoader)\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mLSTM_network\u001b[39;49;00m():\r\n",
      "    \u001b[33m\"\"\"\u001b[39;49;00m\r\n",
      "\u001b[33m    Exceptions are documented in the same way as classes.\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m    The __init__ method may be documented in either the class level\u001b[39;49;00m\r\n",
      "\u001b[33m    docstring, or as a docstring on the __init__ method itself.\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m    Either form is acceptable, but the two should not be mixed. Choose one\u001b[39;49;00m\r\n",
      "\u001b[33m    convention to document the __init__ method and be consistent with it.\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m    Note\u001b[39;49;00m\r\n",
      "\u001b[33m    ----\u001b[39;49;00m\r\n",
      "\u001b[33m    Do not include the `self` parameter in the ``Parameters`` section.\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m    Parameters\u001b[39;49;00m\r\n",
      "\u001b[33m    ----------\u001b[39;49;00m\r\n",
      "\u001b[33m    msg : str\u001b[39;49;00m\r\n",
      "\u001b[33m        Human readable string describing the exception.\u001b[39;49;00m\r\n",
      "\u001b[33m    code : :obj:`int`, optional\u001b[39;49;00m\r\n",
      "\u001b[33m        Numeric error code.\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m    Attributes\u001b[39;49;00m\r\n",
      "\u001b[33m    ----------\u001b[39;49;00m\r\n",
      "\u001b[33m    data_path : str\u001b[39;49;00m\r\n",
      "\u001b[33m        Path to dataset.\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\r\n",
      "\r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m__init__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m):\r\n",
      "\r\n",
      "        \u001b[37m# load variables from the config file\u001b[39;49;00m\r\n",
      "        \u001b[36mself\u001b[39;49;00m.epochs          = variables[\u001b[33m'\u001b[39;49;00m\u001b[33mmodel\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m][\u001b[33m'\u001b[39;49;00m\u001b[33mepochs\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\r\n",
      "        \u001b[36mself\u001b[39;49;00m.batch_size      = variables[\u001b[33m'\u001b[39;49;00m\u001b[33mmodel\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m][\u001b[33m'\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\r\n",
      "        \u001b[36mself\u001b[39;49;00m.hidden_units    = variables[\u001b[33m'\u001b[39;49;00m\u001b[33mmodel\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m][\u001b[33m'\u001b[39;49;00m\u001b[33mhidden_units\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\r\n",
      "        \u001b[36mself\u001b[39;49;00m.gpu_count       = variables[\u001b[33m'\u001b[39;49;00m\u001b[33mmodel\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m][\u001b[33m'\u001b[39;49;00m\u001b[33mgpu_count\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\r\n",
      "        \u001b[36mself\u001b[39;49;00m.model_name      = variables[\u001b[33m'\u001b[39;49;00m\u001b[33mmodel\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m][\u001b[33m'\u001b[39;49;00m\u001b[33mname\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\r\n",
      "        \u001b[36mself\u001b[39;49;00m.data_path       = variables[\u001b[33m'\u001b[39;49;00m\u001b[33mS3\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m][\u001b[33m'\u001b[39;49;00m\u001b[33mdata_path\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\r\n",
      "        \u001b[36mself\u001b[39;49;00m.bucket          = variables[\u001b[33m'\u001b[39;49;00m\u001b[33mS3\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m][\u001b[33m'\u001b[39;49;00m\u001b[33mbucket_name\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\r\n",
      "        \u001b[36mself\u001b[39;49;00m.tokenizer_name  = variables[\u001b[33m'\u001b[39;49;00m\u001b[33mS3\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m][\u001b[33m'\u001b[39;49;00m\u001b[33mtokenizer_name\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\r\n",
      "        \u001b[36mself\u001b[39;49;00m.training_params = variables[\u001b[33m'\u001b[39;49;00m\u001b[33mS3\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m][\u001b[33m'\u001b[39;49;00m\u001b[33mtraining_params\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\r\n",
      "        \u001b[36mself\u001b[39;49;00m.history_pkl     = variables[\u001b[33m'\u001b[39;49;00m\u001b[33mS3\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m][\u001b[33m'\u001b[39;49;00m\u001b[33mhistory_pkl\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\r\n",
      "        \u001b[36mself\u001b[39;49;00m.data_path       = variables[\u001b[33m'\u001b[39;49;00m\u001b[33mdata\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m][\u001b[33m'\u001b[39;49;00m\u001b[33mpath\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\r\n",
      "\r\n",
      "\r\n",
      "        parser = argparse.ArgumentParser()\r\n",
      "\r\n",
      "        parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--epochs\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m10\u001b[39;49;00m)\r\n",
      "        parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--training\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m)\r\n",
      "        \r\n",
      "        args, _ = parser.parse_known_args()\r\n",
      "        \r\n",
      "        \u001b[36mself\u001b[39;49;00m.epochs       = args.epochs\r\n",
      "        \u001b[36mself\u001b[39;49;00m.training_dir = args.training\r\n",
      "        \r\n",
      "\r\n",
      "\r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mdata_load\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, data_location):\r\n",
      "        \u001b[33m\"\"\"\u001b[39;49;00m\r\n",
      "\u001b[33m        Load the data from some remote location.\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m        Parameters\u001b[39;49;00m\r\n",
      "\u001b[33m        ----------\u001b[39;49;00m\r\n",
      "\u001b[33m        data_location : str\u001b[39;49;00m\r\n",
      "\u001b[33m            The path to the password dataset.\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m        Returns\u001b[39;49;00m\r\n",
      "\u001b[33m        -------\u001b[39;49;00m\r\n",
      "\u001b[33m        data\u001b[39;49;00m\r\n",
      "\u001b[33m            The cleaned dataset containing all of the passwords.\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m        \"\"\"\u001b[39;49;00m\r\n",
      "\r\n",
      "\r\n",
      "        \u001b[37m# read the dataset from the S3 bucket and store it as a dask dataframe\u001b[39;49;00m\r\n",
      "        \u001b[37m# with s3.open('%s/%s.parquet' % (self.bucket, self.data_path), 'rb') as f:\u001b[39;49;00m\r\n",
      "        \u001b[37m#     self.data = dd.read_parquet(f)\u001b[39;49;00m\r\n",
      "\r\n",
      "        \u001b[36mself\u001b[39;49;00m.data = pd.read_csv(data_location)\r\n",
      "\r\n",
      "        \u001b[37m# drop the rows with NaN values \u001b[39;49;00m\r\n",
      "        \u001b[36mself\u001b[39;49;00m.data = \u001b[36mself\u001b[39;49;00m.data.dropna()\r\n",
      "\r\n",
      "        \u001b[37m# get rid of duplicate rows\u001b[39;49;00m\r\n",
      "        \u001b[36mself\u001b[39;49;00m.data = \u001b[36mself\u001b[39;49;00m.data.drop_duplicates()\r\n",
      "\r\n",
      "        \u001b[37m# just keep some of the data\u001b[39;49;00m\r\n",
      "        \u001b[36mself\u001b[39;49;00m.data = \u001b[36mself\u001b[39;49;00m.data.head(\u001b[34m1000\u001b[39;49;00m)\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mparse_data\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m):\r\n",
      "        \u001b[33m\"\"\"\u001b[39;49;00m\r\n",
      "\u001b[33m        Parse the data and determine some dataset properties.\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m        Parameters\u001b[39;49;00m\r\n",
      "\u001b[33m        ----------\u001b[39;49;00m\r\n",
      "\u001b[33m        data\u001b[39;49;00m\r\n",
      "\u001b[33m            The cleaned dataset containing all of the passwords.\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m        Returns\u001b[39;49;00m\r\n",
      "\u001b[33m        -------\u001b[39;49;00m\r\n",
      "\u001b[33m        data_length : int\u001b[39;49;00m\r\n",
      "\u001b[33m            The number of passwords in the dataset.\u001b[39;49;00m\r\n",
      "\u001b[33m        unique_characters : int\u001b[39;49;00m\r\n",
      "\u001b[33m            A sorted list of the unique characters in the dataset.\u001b[39;49;00m\r\n",
      "\u001b[33m        vocabulary_size : int\u001b[39;49;00m\r\n",
      "\u001b[33m            The number of unique characters in the dataset.\u001b[39;49;00m\r\n",
      "\u001b[33m        max_length : int\u001b[39;49;00m\r\n",
      "\u001b[33m            The length of the longest password in the dataset.\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m        \"\"\"\u001b[39;49;00m\r\n",
      "\r\n",
      "        \u001b[36mself\u001b[39;49;00m.data_length       = \u001b[36mlen\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m.data)\r\n",
      "        \u001b[36mself\u001b[39;49;00m.unique_characters = \u001b[36msorted\u001b[39;49;00m(\u001b[36mlist\u001b[39;49;00m(\u001b[36mset\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.join(\u001b[36mself\u001b[39;49;00m.data[\u001b[33m'\u001b[39;49;00m\u001b[33mPassword\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]))))\r\n",
      "        \u001b[36mself\u001b[39;49;00m.vocabulary_size   = \u001b[36mlen\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m.unique_characters)\r\n",
      "        \u001b[36mself\u001b[39;49;00m.max_length        = \u001b[36mself\u001b[39;49;00m.data[\u001b[33m'\u001b[39;49;00m\u001b[33mPassword\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m].str.len().max()\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mtokenization\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m):\r\n",
      "        \u001b[33m\"\"\"\u001b[39;49;00m\r\n",
      "\u001b[33m        Parse the data and determine some dataset properties.\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m        Parameters\u001b[39;49;00m\r\n",
      "\u001b[33m        ----------\u001b[39;49;00m\r\n",
      "\u001b[33m        data : pd.DataFrame\u001b[39;49;00m\r\n",
      "\u001b[33m            The dataset containing the passwords.\u001b[39;49;00m\r\n",
      "\u001b[33m        vocabulary_size : int\u001b[39;49;00m\r\n",
      "\u001b[33m            The number of unique characters in the dataset.\u001b[39;49;00m\r\n",
      "\u001b[33m        max_length : int\u001b[39;49;00m\r\n",
      "\u001b[33m            The length of the longest password.\u001b[39;49;00m\r\n",
      "\u001b[33m        bucket : str\u001b[39;49;00m\r\n",
      "\u001b[33m            The name of the S3 bucket in which the results are stored.\u001b[39;49;00m\r\n",
      "\u001b[33m        training_params : str\u001b[39;49;00m\r\n",
      "\u001b[33m            The name of the pickle object to store in S3.\u001b[39;49;00m\r\n",
      "\u001b[33m        tokenizer_name : str\u001b[39;49;00m\r\n",
      "\u001b[33m            The name of the tokenizer object to be store in S3.\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m        Returns\u001b[39;49;00m\r\n",
      "\u001b[33m        -------\u001b[39;49;00m\r\n",
      "\u001b[33m        tokenizer : \u001b[39;49;00m\r\n",
      "\u001b[33m            The Keras tokenizer object.\u001b[39;49;00m\r\n",
      "\u001b[33m        character_to_ix : \u001b[39;49;00m\r\n",
      "\u001b[33m            The character-to-index dictionary.\u001b[39;49;00m\r\n",
      "\u001b[33m        ix_to_character : \u001b[39;49;00m\r\n",
      "\u001b[33m            The index-to-character dictionary.\u001b[39;49;00m\r\n",
      "\u001b[33m        data : pd.DataFrame\u001b[39;49;00m\r\n",
      "\u001b[33m            The dataset, including the tokenized passwords.\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m        \"\"\"\u001b[39;49;00m\r\n",
      "\r\n",
      "        \u001b[37m# get the password column as its own array\u001b[39;49;00m\r\n",
      "        passwords = \u001b[36mself\u001b[39;49;00m.data[\u001b[33m'\u001b[39;49;00m\u001b[33mPassword\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\r\n",
      "\r\n",
      "        \u001b[37m# define the tokenizer \u001b[39;49;00m\r\n",
      "        \u001b[36mself\u001b[39;49;00m.tokenizer = Tokenizer(num_words=\u001b[36mNone\u001b[39;49;00m, oov_token=\u001b[33m'\u001b[39;49;00m\u001b[33mUNK\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, char_level=\u001b[36mTrue\u001b[39;49;00m)\r\n",
      "\r\n",
      "        \u001b[37m# generate the tokenized passwords      \u001b[39;49;00m\r\n",
      "        \u001b[36mself\u001b[39;49;00m.tokenizer.fit_on_texts(passwords)\r\n",
      "\r\n",
      "        \u001b[37m# generate the character-to-index dictionary \u001b[39;49;00m\r\n",
      "        \u001b[36mself\u001b[39;49;00m.character_to_ix = \u001b[36mself\u001b[39;49;00m.tokenizer.word_index\r\n",
      "\r\n",
      "        \u001b[37m# generate the index-to-character dictionary too\u001b[39;49;00m\r\n",
      "        \u001b[36mself\u001b[39;49;00m.ix_to_character = {i: j \u001b[34mfor\u001b[39;49;00m j, i \u001b[35min\u001b[39;49;00m \u001b[36mself\u001b[39;49;00m.character_to_ix.items()}\r\n",
      "\r\n",
      "        \u001b[37m# persist the tokenizer\u001b[39;49;00m\r\n",
      "        \u001b[37m# with s3.open('%s/%s' % (self.bucket, self.tokenizer_name), 'w') as f:\u001b[39;49;00m\r\n",
      "        \u001b[37m#     f.write(json.dumps(self.tokenizer.to_json(), ensure_ascii=False))\u001b[39;49;00m\r\n",
      "\r\n",
      "        \u001b[37m# save the index-to-character dictionary and self.vocabulary_size values\u001b[39;49;00m\r\n",
      "        \u001b[37m# with s3.open('%s/%s' % (self.bucket, self.training_params), 'wb') as f:\u001b[39;49;00m\r\n",
      "        \u001b[37m#     pickle.dump([self.ix_to_character, self.vocabulary_size, self.max_length], f)\u001b[39;49;00m\r\n",
      "\r\n",
      "        \u001b[37m# this encodes the passwords\u001b[39;49;00m\r\n",
      "        tokens = \u001b[36mself\u001b[39;49;00m.tokenizer.texts_to_sequences(passwords)\r\n",
      "\r\n",
      "        \u001b[37m# save the tokenized passwords in a column of the dataframe\u001b[39;49;00m\r\n",
      "        \u001b[36mself\u001b[39;49;00m.data[\u001b[33m'\u001b[39;49;00m\u001b[33mTokenized\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] = tokens\r\n",
      "\r\n",
      "        \u001b[37m# turn the tokenized column into a column of arrays (not lists)\u001b[39;49;00m\r\n",
      "        \u001b[36mself\u001b[39;49;00m.data[\u001b[33m'\u001b[39;49;00m\u001b[33mTokenized\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] = \u001b[36mself\u001b[39;49;00m.data[\u001b[33m'\u001b[39;49;00m\u001b[33mTokenized\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m].apply(\u001b[34mlambda\u001b[39;49;00m x: np.array(x))\r\n",
      "\r\n",
      "        \u001b[37m# this gets rid of the <PAD> character\u001b[39;49;00m\r\n",
      "        \u001b[36mself\u001b[39;49;00m.data[\u001b[33m'\u001b[39;49;00m\u001b[33mOutput\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] = \u001b[36mself\u001b[39;49;00m.data[\u001b[33m'\u001b[39;49;00m\u001b[33mTokenized\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] - \u001b[34m1\u001b[39;49;00m\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mmodel_construction\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m):\r\n",
      "        \u001b[33m\"\"\"\u001b[39;49;00m\r\n",
      "\u001b[33m        Construct the model.\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m        Parameters\u001b[39;49;00m\r\n",
      "\u001b[33m        ----------\u001b[39;49;00m\r\n",
      "\u001b[33m        vocabulary_size\u001b[39;49;00m\r\n",
      "\u001b[33m            The number of unique characters in the dataset.\u001b[39;49;00m\r\n",
      "\u001b[33m        max_length\u001b[39;49;00m\r\n",
      "\u001b[33m            The length of the longest password.\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m        Outputs\u001b[39;49;00m\r\n",
      "\u001b[33m        -------\u001b[39;49;00m\r\n",
      "\u001b[33m        model : \u001b[39;49;00m\r\n",
      "\u001b[33m            The Keras model.\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m        \"\"\"\u001b[39;49;00m\r\n",
      "\r\n",
      "\r\n",
      "        \u001b[37m# build the model\u001b[39;49;00m\r\n",
      "        \u001b[36mself\u001b[39;49;00m.model = Sequential()\r\n",
      "        \u001b[36mself\u001b[39;49;00m.model.add(Embedding(input_dim=\u001b[36mself\u001b[39;49;00m.vocabulary_size + \u001b[34m1\u001b[39;49;00m,             \u001b[37m# vocabulary size plus an extra element for <PAD> \u001b[39;49;00m\r\n",
      "                                output_dim=\u001b[36mint\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m.vocabulary_size ** (\u001b[34m1.\u001b[39;49;00m/\u001b[34m4\u001b[39;49;00m)),  \u001b[37m# size of embeddings; fourth root of cardinality\u001b[39;49;00m\r\n",
      "                                input_length=\u001b[36mself\u001b[39;49;00m.max_length - \u001b[34m1\u001b[39;49;00m))               \u001b[37m# length of the padded sequences\u001b[39;49;00m\r\n",
      "        \u001b[36mself\u001b[39;49;00m.model.add(Bidirectional(LSTM(\u001b[34m50\u001b[39;49;00m)))                                  \u001b[37m# size of hidden layer; n_h ~= n_s / (2(n_i + n_o)) \u001b[39;49;00m\r\n",
      "        \u001b[36mself\u001b[39;49;00m.model.add(Dense(\u001b[36mself\u001b[39;49;00m.vocabulary_size, activation=\u001b[33m'\u001b[39;49;00m\u001b[33msoftmax\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))        \u001b[37m# output\u001b[39;49;00m\r\n",
      "        \u001b[36mself\u001b[39;49;00m.model.compile(\u001b[33m'\u001b[39;49;00m\u001b[33mrmsprop\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mcategorical_crossentropy\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "        \u001b[34mprint\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m.model.summary())\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mmodel_training\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m):\r\n",
      "        \u001b[33m\"\"\"\u001b[39;49;00m\r\n",
      "\u001b[33m        Train the model.\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m        Parameters\u001b[39;49;00m\r\n",
      "\u001b[33m        ----------\u001b[39;49;00m\r\n",
      "\u001b[33m        data : pd.DataFrame\u001b[39;49;00m\r\n",
      "\u001b[33m            The dataset containing the passwords.\u001b[39;49;00m\r\n",
      "\u001b[33m        vocabulary_size : int\u001b[39;49;00m\r\n",
      "\u001b[33m            The number of unique characters in the dataset.\u001b[39;49;00m\r\n",
      "\u001b[33m        max_length : int\u001b[39;49;00m\r\n",
      "\u001b[33m            The length of the longest password.\u001b[39;49;00m\r\n",
      "\u001b[33m        batch_size : int\u001b[39;49;00m\r\n",
      "\u001b[33m            The number of samples to train during a single iteration.\u001b[39;49;00m\r\n",
      "\u001b[33m        epoch_size : int\u001b[39;49;00m\r\n",
      "\u001b[33m            The number of steps to train the model.\u001b[39;49;00m\r\n",
      "\u001b[33m        model : \u001b[39;49;00m\r\n",
      "\u001b[33m            The Keras model created in model_construction.\u001b[39;49;00m\r\n",
      "\u001b[33m        bucket : str\u001b[39;49;00m\r\n",
      "\u001b[33m            The name of the S3 bucket in which the results are stored.\u001b[39;49;00m\r\n",
      "\u001b[33m        history_pkl : str\u001b[39;49;00m\r\n",
      "\u001b[33m            The name of the pickle object to store in S3.\u001b[39;49;00m\r\n",
      "\u001b[33m        model_name : str\u001b[39;49;00m\r\n",
      "\u001b[33m            The name of the model to be store in S3.\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m        Returns\u001b[39;49;00m\r\n",
      "\u001b[33m        -------\u001b[39;49;00m\r\n",
      "\u001b[33m        history : \u001b[39;49;00m\r\n",
      "\u001b[33m            The Keras history object.\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m        \"\"\"\u001b[39;49;00m\r\n",
      "\r\n",
      "        \u001b[37m# define the generator parameters\u001b[39;49;00m\r\n",
      "        paramaters = {\u001b[33m'\u001b[39;49;00m\u001b[33mvocabulary_size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: \u001b[36mself\u001b[39;49;00m.vocabulary_size,\r\n",
      "                      \u001b[33m'\u001b[39;49;00m\u001b[33mmax_length\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:      \u001b[36mself\u001b[39;49;00m.max_length,\r\n",
      "                      \u001b[33m'\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:      \u001b[36mself\u001b[39;49;00m.batch_size,\r\n",
      "                      \u001b[33m'\u001b[39;49;00m\u001b[33mshuffle\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:         \u001b[36mTrue\u001b[39;49;00m}\r\n",
      "\r\n",
      "        \u001b[37m# split the data into training and testing sets\u001b[39;49;00m\r\n",
      "        training, testing = train_test_split(\u001b[36mself\u001b[39;49;00m.data, test_size=\u001b[34m0.1\u001b[39;49;00m)\r\n",
      " \r\n",
      "        \u001b[37m# check memory\u001b[39;49;00m\r\n",
      "        \u001b[34mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mthese are the memory stats prior to training: \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "        \u001b[34mprint\u001b[39;49;00m(psutil.virtual_memory())\r\n",
      "\r\n",
      "        \u001b[34mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mstarting training of model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "\r\n",
      "        \u001b[37m# define the generators for the training and test datasets\u001b[39;49;00m\r\n",
      "        training_generator = DataGenerator(training, **paramaters)\r\n",
      "        test_generator     = DataGenerator(testing, **paramaters)\r\n",
      "        \u001b[34mprint\u001b[39;49;00m(psutil.virtual_memory())\r\n",
      "\r\n",
      "        \u001b[37m# callbacks during training\u001b[39;49;00m\r\n",
      "        save_checkpoint = ModelCheckpoint(\u001b[33m'\u001b[39;49;00m\u001b[33m%s\u001b[39;49;00m\u001b[33m.h5\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m % \u001b[36mself\u001b[39;49;00m.model_name, monitor=\u001b[33m'\u001b[39;49;00m\u001b[33mval_acc\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, save_best_only=\u001b[36mTrue\u001b[39;49;00m)\r\n",
      "        early_stopping  = EarlyStopping(monitor=\u001b[33m'\u001b[39;49;00m\u001b[33mloss\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, patience=\u001b[34m5\u001b[39;49;00m)\r\n",
      "\r\n",
      "        \u001b[37m# add support for multiple GPUs\u001b[39;49;00m\r\n",
      "        \u001b[34mif\u001b[39;49;00m \u001b[36mself\u001b[39;49;00m.gpu_count > \u001b[34m1\u001b[39;49;00m:\r\n",
      "            \u001b[36mself\u001b[39;49;00m.model = multi_gpu_model(\u001b[36mself\u001b[39;49;00m.model, gpus=\u001b[36mself\u001b[39;49;00m.gpu_count)\r\n",
      "\r\n",
      "        \u001b[37m# train the network\u001b[39;49;00m\r\n",
      "        \u001b[36mself\u001b[39;49;00m.history = \u001b[36mself\u001b[39;49;00m.model.fit_generator(generator=training_generator,\r\n",
      "                                 validation_data=test_generator,\r\n",
      "                                 epochs=\u001b[36mself\u001b[39;49;00m.epochs, \r\n",
      "                                 steps_per_epoch=(\u001b[36mlen\u001b[39;49;00m(training) // \u001b[36mself\u001b[39;49;00m.batch_size),\r\n",
      "                                 validation_steps=(\u001b[36mlen\u001b[39;49;00m(testing) // \u001b[36mself\u001b[39;49;00m.batch_size),\r\n",
      "                                 callbacks=[save_checkpoint, early_stopping],\r\n",
      "                                 use_multiprocessing=\u001b[36mTrue\u001b[39;49;00m,\r\n",
      "                                 workers=multiprocessing.cpu_count(),\r\n",
      "                                 max_queue_size=multiprocessing.cpu_count()*\u001b[34m2\u001b[39;49;00m,\r\n",
      "                                 verbose=\u001b[34m1\u001b[39;49;00m).history\r\n",
      "\r\n",
      "        \u001b[37m# save the history variable\u001b[39;49;00m\r\n",
      "        \u001b[37m# with s3.open('%s/%s.pkl' % (self.bucket, self.history_pkl), 'wb') as f:\u001b[39;49;00m\r\n",
      "        \u001b[37m#     pickle.dump(self.history, f)\u001b[39;49;00m\r\n",
      "        \r\n",
      "        \u001b[37m# save the model in an S3 bucket\u001b[39;49;00m\r\n",
      "        \u001b[36mself\u001b[39;49;00m.model.save(\u001b[33m'\u001b[39;49;00m\u001b[33m%s\u001b[39;49;00m\u001b[33m.h5\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m % \u001b[36mself\u001b[39;49;00m.model_name)\r\n",
      "        \u001b[37m# with open('%s.h5' % self.model_name, \"rb\") as f:\u001b[39;49;00m\r\n",
      "        \u001b[37m#     client.upload_fileobj(Fileobj=f, \u001b[39;49;00m\r\n",
      "        \u001b[37m#                           Bucket=self.bucket, \u001b[39;49;00m\r\n",
      "        \u001b[37m#                           Key='%s.h5' % self.model_name)\u001b[39;49;00m\r\n",
      "\r\n",
      "\r\n",
      "        \u001b[34mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mfinished training model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mpassword_probability\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, password):\r\n",
      "        \u001b[33m\"\"\"\u001b[39;49;00m\r\n",
      "\u001b[33m        Calculate the probability of a given password.\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m        Parameters\u001b[39;49;00m\r\n",
      "\u001b[33m        ----------\u001b[39;49;00m\r\n",
      "\u001b[33m        password : str\u001b[39;49;00m\r\n",
      "\u001b[33m            The password whose probability is to be calculated.\u001b[39;49;00m\r\n",
      "\u001b[33m        model : \u001b[39;49;00m\r\n",
      "\u001b[33m            The Keras model.\u001b[39;49;00m\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m        tokenizer : \u001b[39;49;00m\r\n",
      "\u001b[33m            The Keras tokenizer object.\u001b[39;49;00m\r\n",
      "\u001b[33m        ix_to_character : \u001b[39;49;00m\r\n",
      "\u001b[33m            The c-to-character dictionary.\u001b[39;49;00m\r\n",
      "\u001b[33m        data : pd.DataFrame\u001b[39;49;00m\r\n",
      "\u001b[33m            The dataset, including the tokenized passwords.\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m        Returns\u001b[39;49;00m\r\n",
      "\u001b[33m        -------\u001b[39;49;00m\r\n",
      "\u001b[33m        float\u001b[39;49;00m\r\n",
      "\u001b[33m            The probability of the password.\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m        \"\"\"\u001b[39;49;00m\r\n",
      "\r\n",
      "        \u001b[37m# tokenize the password\u001b[39;49;00m\r\n",
      "        token  = \u001b[36mself\u001b[39;49;00m.tokenizer.texts_to_sequences([password])[\u001b[34m0\u001b[39;49;00m]\r\n",
      "        x_test = DataGenerator.slide_window(token)\r\n",
      "        x_test = np.array(x_test)\r\n",
      "        y_test = token - \u001b[34m1\u001b[39;49;00m\r\n",
      "\r\n",
      "        \u001b[37m# determine the probabilities of the permutations of the words\u001b[39;49;00m\r\n",
      "        probabilities = \u001b[36mself\u001b[39;49;00m.model.predict(x_test, verbose=\u001b[34m0\u001b[39;49;00m)\r\n",
      "\r\n",
      "        \u001b[37m# calculate the probability of the password\u001b[39;49;00m\r\n",
      "        password_probability = \u001b[34m0\u001b[39;49;00m\r\n",
      "        \u001b[34mfor\u001b[39;49;00m index, probability \u001b[35min\u001b[39;49;00m \u001b[36menumerate\u001b[39;49;00m(probabilities):\r\n",
      "            word                  = \u001b[36mself\u001b[39;49;00m.ix_to_character[y_test[index] + \u001b[34m1\u001b[39;49;00m]  \u001b[37m# the first element is <PAD>\u001b[39;49;00m\r\n",
      "            word_probability      = probability[y_test[index]]               \u001b[37m# get the probability from the model\u001b[39;49;00m\r\n",
      "            password_probability += np.log(word_probability)                 \u001b[37m# use log to avoid roundoff errors\u001b[39;49;00m\r\n",
      "\r\n",
      "        \u001b[37m# calculate the perplexity to account for varying password lengths\u001b[39;49;00m\r\n",
      "        password_length       = \u001b[36mlen\u001b[39;49;00m(password)    \r\n",
      "        password_probability /= -password_length\r\n",
      "        password_probability  = np.exp(password_probability)  \u001b[37m# recover the raw probability\u001b[39;49;00m\r\n",
      "\r\n",
      "\r\n",
      "        \u001b[34mreturn\u001b[39;49;00m password_probability\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mguess\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, sequence, alterations, additions, top):\r\n",
      "        \u001b[33m\"\"\"\u001b[39;49;00m\r\n",
      "\u001b[33m        Calculate the probability of a given sequence.\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m        Parameters\u001b[39;49;00m\r\n",
      "\u001b[33m        ----------\u001b[39;49;00m\r\n",
      "\u001b[33m        sequence\u001b[39;49;00m\r\n",
      "\u001b[33m            The sequence to be altered.\u001b[39;49;00m\r\n",
      "\u001b[33m        alterations\u001b[39;49;00m\r\n",
      "\u001b[33m            The number of characters in the input sequence to alter.\u001b[39;49;00m\r\n",
      "\u001b[33m        additions\u001b[39;49;00m\r\n",
      "\u001b[33m            The number of characters to add to the sequence.\u001b[39;49;00m\r\n",
      "\u001b[33m        top\u001b[39;49;00m\r\n",
      "\u001b[33m            The number of most likely candidates to return.\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m        Returns\u001b[39;49;00m\r\n",
      "\u001b[33m        -------\u001b[39;49;00m\r\n",
      "\u001b[33m        dict\u001b[39;49;00m\r\n",
      "\u001b[33m            Keys correspond to the number of alterations.\u001b[39;49;00m\r\n",
      "\u001b[33m            Values correspond to the top N guesses.\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m        \"\"\"\u001b[39;49;00m\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\u001b[37m# run the program\u001b[39;49;00m\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mmain\u001b[39;49;00m():\r\n",
      "\r\n",
      "    \u001b[37m# instantiate the class\u001b[39;49;00m\r\n",
      "    l = LSTM_network()\r\n",
      "\r\n",
      "    \u001b[37m# load the data\u001b[39;49;00m\r\n",
      "    l.data_load(l.training_dir)\r\n",
      "\r\n",
      "    \u001b[37m# # get the dataset characteristics\u001b[39;49;00m\r\n",
      "    \u001b[37m# l.parse_data()    \u001b[39;49;00m\r\n",
      "\r\n",
      "    \u001b[37m# # tokenize the data\u001b[39;49;00m\r\n",
      "    \u001b[37m# l.tokenization()\u001b[39;49;00m\r\n",
      "\r\n",
      "    \u001b[37m# # initialize the model\u001b[39;49;00m\r\n",
      "    \u001b[37m# l.model_construction()\u001b[39;49;00m\r\n",
      "\r\n",
      "    \u001b[37m# # train the model\u001b[39;49;00m\r\n",
      "    \u001b[37m# l.model_training()\u001b[39;49;00m\r\n",
      "    \u001b[34mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mhello\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m\"\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\r\n",
      "    main()\r\n"
     ]
    }
   ],
   "source": [
    "!pygmentize lstm_cracker/program/program.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_estimator = TensorFlow(entry_point='program.py', \n",
    "                          role=role,\n",
    "                          source_dir='lstm_cracker/program',\n",
    "                          model_dir=output_path,\n",
    "                          train_instance_count=1, \n",
    "                          train_instance_type='local',\n",
    "                          framework_version='1.12', \n",
    "                          py_version='py3',\n",
    "                          script_mode=True,\n",
    "                          hyperparameters={'epochs': 1, 'training': s3_train_data}\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating tmpvs3dqwvv_algo-1-vws6y_1 ... \n",
      "\u001b[1BAttaching to tmpvs3dqwvv_algo-1-vws6y_12mdone\u001b[0m\n",
      "\u001b[36malgo-1-vws6y_1  |\u001b[0m 2019-11-27 20:25:53,245 sagemaker-containers INFO     Imported framework sagemaker_tensorflow_container.training\n",
      "\u001b[36malgo-1-vws6y_1  |\u001b[0m 2019-11-27 20:25:53,251 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\n",
      "\u001b[36malgo-1-vws6y_1  |\u001b[0m 2019-11-27 20:25:53,429 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\n",
      "\u001b[36malgo-1-vws6y_1  |\u001b[0m 2019-11-27 20:25:53,448 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\n",
      "\u001b[36malgo-1-vws6y_1  |\u001b[0m 2019-11-27 20:25:53,462 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[36malgo-1-vws6y_1  |\u001b[0m \n",
      "\u001b[36malgo-1-vws6y_1  |\u001b[0m Training Env:\n",
      "\u001b[36malgo-1-vws6y_1  |\u001b[0m \n",
      "\u001b[36malgo-1-vws6y_1  |\u001b[0m {\n",
      "\u001b[36malgo-1-vws6y_1  |\u001b[0m     \"additional_framework_parameters\": {},\n",
      "\u001b[36malgo-1-vws6y_1  |\u001b[0m     \"channel_input_dirs\": {\n",
      "\u001b[36malgo-1-vws6y_1  |\u001b[0m         \"training\": \"/opt/ml/input/data/training\"\n",
      "\u001b[36malgo-1-vws6y_1  |\u001b[0m     },\n",
      "\u001b[36malgo-1-vws6y_1  |\u001b[0m     \"current_host\": \"algo-1-vws6y\",\n",
      "\u001b[36malgo-1-vws6y_1  |\u001b[0m     \"framework_module\": \"sagemaker_tensorflow_container.training:main\",\n",
      "\u001b[36malgo-1-vws6y_1  |\u001b[0m     \"hosts\": [\n",
      "\u001b[36malgo-1-vws6y_1  |\u001b[0m         \"algo-1-vws6y\"\n",
      "\u001b[36malgo-1-vws6y_1  |\u001b[0m     ],\n",
      "\u001b[36malgo-1-vws6y_1  |\u001b[0m     \"hyperparameters\": {\n",
      "\u001b[36malgo-1-vws6y_1  |\u001b[0m         \"epochs\": 1,\n",
      "\u001b[36malgo-1-vws6y_1  |\u001b[0m         \"training\": \"s3://blstm-cracker/test-run/train/train.csv\",\n",
      "\u001b[36malgo-1-vws6y_1  |\u001b[0m         \"model_dir\": \"s3://blstm-cracker/test-run/output\"\n",
      "\u001b[36malgo-1-vws6y_1  |\u001b[0m     },\n",
      "\u001b[36malgo-1-vws6y_1  |\u001b[0m     \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "\u001b[36malgo-1-vws6y_1  |\u001b[0m     \"input_data_config\": {\n",
      "\u001b[36malgo-1-vws6y_1  |\u001b[0m         \"training\": {\n",
      "\u001b[36malgo-1-vws6y_1  |\u001b[0m             \"TrainingInputMode\": \"File\",\n",
      "\u001b[36malgo-1-vws6y_1  |\u001b[0m             \"ContentType\": \"text/csv\"\n",
      "\u001b[36malgo-1-vws6y_1  |\u001b[0m         }\n",
      "\u001b[36malgo-1-vws6y_1  |\u001b[0m     },\n",
      "\u001b[36malgo-1-vws6y_1  |\u001b[0m     \"input_dir\": \"/opt/ml/input\",\n",
      "\u001b[36malgo-1-vws6y_1  |\u001b[0m     \"is_master\": true,\n",
      "\u001b[36malgo-1-vws6y_1  |\u001b[0m     \"job_name\": \"sagemaker-tensorflow-scriptmode-2019-11-27-20-25-49-799\",\n",
      "\u001b[36malgo-1-vws6y_1  |\u001b[0m     \"log_level\": 20,\n",
      "\u001b[36malgo-1-vws6y_1  |\u001b[0m     \"master_hostname\": \"algo-1-vws6y\",\n",
      "\u001b[36malgo-1-vws6y_1  |\u001b[0m     \"model_dir\": \"/opt/ml/model\",\n",
      "\u001b[36malgo-1-vws6y_1  |\u001b[0m     \"module_dir\": \"s3://sagemaker-us-east-2-982075062069/sagemaker-tensorflow-scriptmode-2019-11-27-20-25-49-799/source/sourcedir.tar.gz\",\n",
      "\u001b[36malgo-1-vws6y_1  |\u001b[0m     \"module_name\": \"program\",\n",
      "\u001b[36malgo-1-vws6y_1  |\u001b[0m     \"network_interface_name\": \"eth0\",\n",
      "\u001b[36malgo-1-vws6y_1  |\u001b[0m     \"num_cpus\": 2,\n",
      "\u001b[36malgo-1-vws6y_1  |\u001b[0m     \"num_gpus\": 0,\n",
      "\u001b[36malgo-1-vws6y_1  |\u001b[0m     \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "\u001b[36malgo-1-vws6y_1  |\u001b[0m     \"output_dir\": \"/opt/ml/output\",\n",
      "\u001b[36malgo-1-vws6y_1  |\u001b[0m     \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "\u001b[36malgo-1-vws6y_1  |\u001b[0m     \"resource_config\": {\n",
      "\u001b[36malgo-1-vws6y_1  |\u001b[0m         \"current_host\": \"algo-1-vws6y\",\n",
      "\u001b[36malgo-1-vws6y_1  |\u001b[0m         \"hosts\": [\n",
      "\u001b[36malgo-1-vws6y_1  |\u001b[0m             \"algo-1-vws6y\"\n",
      "\u001b[36malgo-1-vws6y_1  |\u001b[0m         ]\n",
      "\u001b[36malgo-1-vws6y_1  |\u001b[0m     },\n",
      "\u001b[36malgo-1-vws6y_1  |\u001b[0m     \"user_entry_point\": \"program.py\"\n",
      "\u001b[36malgo-1-vws6y_1  |\u001b[0m }\n",
      "\u001b[36malgo-1-vws6y_1  |\u001b[0m \n",
      "\u001b[36malgo-1-vws6y_1  |\u001b[0m Environment variables:\n",
      "\u001b[36malgo-1-vws6y_1  |\u001b[0m \n",
      "\u001b[36malgo-1-vws6y_1  |\u001b[0m SM_HOSTS=[\"algo-1-vws6y\"]\n",
      "\u001b[36malgo-1-vws6y_1  |\u001b[0m SM_NETWORK_INTERFACE_NAME=eth0\n",
      "\u001b[36malgo-1-vws6y_1  |\u001b[0m SM_HPS={\"epochs\":1,\"model_dir\":\"s3://blstm-cracker/test-run/output\",\"training\":\"s3://blstm-cracker/test-run/train/train.csv\"}\n",
      "\u001b[36malgo-1-vws6y_1  |\u001b[0m SM_USER_ENTRY_POINT=program.py\n",
      "\u001b[36malgo-1-vws6y_1  |\u001b[0m SM_FRAMEWORK_PARAMS={}\n",
      "\u001b[36malgo-1-vws6y_1  |\u001b[0m SM_RESOURCE_CONFIG={\"current_host\":\"algo-1-vws6y\",\"hosts\":[\"algo-1-vws6y\"]}\n",
      "\u001b[36malgo-1-vws6y_1  |\u001b[0m SM_INPUT_DATA_CONFIG={\"training\":{\"ContentType\":\"text/csv\",\"TrainingInputMode\":\"File\"}}\n",
      "\u001b[36malgo-1-vws6y_1  |\u001b[0m SM_OUTPUT_DATA_DIR=/opt/ml/output/data\n",
      "\u001b[36malgo-1-vws6y_1  |\u001b[0m SM_CHANNELS=[\"training\"]\n",
      "\u001b[36malgo-1-vws6y_1  |\u001b[0m SM_CURRENT_HOST=algo-1-vws6y\n",
      "\u001b[36malgo-1-vws6y_1  |\u001b[0m SM_MODULE_NAME=program\n",
      "\u001b[36malgo-1-vws6y_1  |\u001b[0m SM_LOG_LEVEL=20\n",
      "\u001b[36malgo-1-vws6y_1  |\u001b[0m SM_FRAMEWORK_MODULE=sagemaker_tensorflow_container.training:main\n",
      "\u001b[36malgo-1-vws6y_1  |\u001b[0m SM_INPUT_DIR=/opt/ml/input\n",
      "\u001b[36malgo-1-vws6y_1  |\u001b[0m SM_INPUT_CONFIG_DIR=/opt/ml/input/config\n",
      "\u001b[36malgo-1-vws6y_1  |\u001b[0m SM_OUTPUT_DIR=/opt/ml/output\n",
      "\u001b[36malgo-1-vws6y_1  |\u001b[0m SM_NUM_CPUS=2\n",
      "\u001b[36malgo-1-vws6y_1  |\u001b[0m SM_NUM_GPUS=0\n",
      "\u001b[36malgo-1-vws6y_1  |\u001b[0m SM_MODEL_DIR=/opt/ml/model\n",
      "\u001b[36malgo-1-vws6y_1  |\u001b[0m SM_MODULE_DIR=s3://sagemaker-us-east-2-982075062069/sagemaker-tensorflow-scriptmode-2019-11-27-20-25-49-799/source/sourcedir.tar.gz\n",
      "\u001b[36malgo-1-vws6y_1  |\u001b[0m SM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1-vws6y\",\"framework_module\":\"sagemaker_tensorflow_container.training:main\",\"hosts\":[\"algo-1-vws6y\"],\"hyperparameters\":{\"epochs\":1,\"model_dir\":\"s3://blstm-cracker/test-run/output\",\"training\":\"s3://blstm-cracker/test-run/train/train.csv\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"ContentType\":\"text/csv\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"sagemaker-tensorflow-scriptmode-2019-11-27-20-25-49-799\",\"log_level\":20,\"master_hostname\":\"algo-1-vws6y\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-2-982075062069/sagemaker-tensorflow-scriptmode-2019-11-27-20-25-49-799/source/sourcedir.tar.gz\",\"module_name\":\"program\",\"network_interface_name\":\"eth0\",\"num_cpus\":2,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1-vws6y\",\"hosts\":[\"algo-1-vws6y\"]},\"user_entry_point\":\"program.py\"}\n",
      "\u001b[36malgo-1-vws6y_1  |\u001b[0m SM_USER_ARGS=[\"--epochs\",\"1\",\"--model_dir\",\"s3://blstm-cracker/test-run/output\",\"--training\",\"s3://blstm-cracker/test-run/train/train.csv\"]\n",
      "\u001b[36malgo-1-vws6y_1  |\u001b[0m SM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\n",
      "\u001b[36malgo-1-vws6y_1  |\u001b[0m SM_CHANNEL_TRAINING=/opt/ml/input/data/training\n",
      "\u001b[36malgo-1-vws6y_1  |\u001b[0m SM_HP_EPOCHS=1\n",
      "\u001b[36malgo-1-vws6y_1  |\u001b[0m SM_HP_TRAINING=s3://blstm-cracker/test-run/train/train.csv\n",
      "\u001b[36malgo-1-vws6y_1  |\u001b[0m SM_HP_MODEL_DIR=s3://blstm-cracker/test-run/output\n",
      "\u001b[36malgo-1-vws6y_1  |\u001b[0m PYTHONPATH=/opt/ml/code:/usr/local/bin:/usr/lib/python36.zip:/usr/lib/python3.6:/usr/lib/python3.6/lib-dynload:/usr/local/lib/python3.6/dist-packages:/usr/lib/python3/dist-packages\n",
      "\u001b[36malgo-1-vws6y_1  |\u001b[0m \n",
      "\u001b[36malgo-1-vws6y_1  |\u001b[0m Invoking script with the following command:\n",
      "\u001b[36malgo-1-vws6y_1  |\u001b[0m \n",
      "\u001b[36malgo-1-vws6y_1  |\u001b[0m /usr/bin/python program.py --epochs 1 --model_dir s3://blstm-cracker/test-run/output --training s3://blstm-cracker/test-run/train/train.csv\n",
      "\u001b[36malgo-1-vws6y_1  |\u001b[0m \n",
      "\u001b[36malgo-1-vws6y_1  |\u001b[0m \n",
      "\u001b[36malgo-1-vws6y_1  |\u001b[0m Using TensorFlow backend.\n",
      "\u001b[36malgo-1-vws6y_1  |\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36malgo-1-vws6y_1  |\u001b[0m   File \"/usr/local/lib/python3.6/dist-packages/pandas/io/s3.py\", line 5, in <module>\n",
      "\u001b[36malgo-1-vws6y_1  |\u001b[0m     import s3fs\n",
      "\u001b[36malgo-1-vws6y_1  |\u001b[0m ModuleNotFoundError: No module named 's3fs'\n",
      "\u001b[36malgo-1-vws6y_1  |\u001b[0m \n",
      "\u001b[36malgo-1-vws6y_1  |\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[36malgo-1-vws6y_1  |\u001b[0m \n",
      "\u001b[36malgo-1-vws6y_1  |\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36malgo-1-vws6y_1  |\u001b[0m   File \"program.py\", line 545, in <module>\n",
      "\u001b[36malgo-1-vws6y_1  |\u001b[0m     main()\n",
      "\u001b[36malgo-1-vws6y_1  |\u001b[0m   File \"program.py\", line 527, in main\n",
      "\u001b[36malgo-1-vws6y_1  |\u001b[0m     l.data_load(l.training_dir)\n",
      "\u001b[36malgo-1-vws6y_1  |\u001b[0m   File \"program.py\", line 192, in data_load\n",
      "\u001b[36malgo-1-vws6y_1  |\u001b[0m     self.data = pd.read_csv(data_location)\n",
      "\u001b[36malgo-1-vws6y_1  |\u001b[0m   File \"/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\", line 702, in parser_f\n",
      "\u001b[36malgo-1-vws6y_1  |\u001b[0m     return _read(filepath_or_buffer, kwds)\n",
      "\u001b[36malgo-1-vws6y_1  |\u001b[0m   File \"/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\", line 413, in _read\n",
      "\u001b[36malgo-1-vws6y_1  |\u001b[0m     filepath_or_buffer, encoding, compression)\n",
      "\u001b[36malgo-1-vws6y_1  |\u001b[0m   File \"/usr/local/lib/python3.6/dist-packages/pandas/io/common.py\", line 212, in get_filepath_or_buffer\n",
      "\u001b[36malgo-1-vws6y_1  |\u001b[0m     from pandas.io import s3\n",
      "\u001b[36malgo-1-vws6y_1  |\u001b[0m   File \"/usr/local/lib/python3.6/dist-packages/pandas/io/s3.py\", line 8, in <module>\n",
      "\u001b[36malgo-1-vws6y_1  |\u001b[0m     raise ImportError(\"The s3fs library is required to handle s3 files\")\n",
      "\u001b[36malgo-1-vws6y_1  |\u001b[0m ImportError: The s3fs library is required to handle s3 files\n",
      "\u001b[36malgo-1-vws6y_1  |\u001b[0m 2019-11-27 20:25:55,316 sagemaker-containers ERROR    ExecuteUserScriptError:\n",
      "\u001b[36malgo-1-vws6y_1  |\u001b[0m Command \"/usr/bin/python program.py --epochs 1 --model_dir s3://blstm-cracker/test-run/output --training s3://blstm-cracker/test-run/train/train.csv\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36mtmpvs3dqwvv_algo-1-vws6y_1 exited with code 1\n",
      "\u001b[0mAborting on container exit...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Failed to run: ['docker-compose', '-f', '/tmp/tmpvs3dqwvv/docker-compose.yaml', 'up', '--build', '--abort-on-container-exit'], Process exited with code: 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/sagemaker/local/image.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_data_config, output_data_config, hyperparameters, job_name)\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m             \u001b[0m_stream_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/sagemaker/local/image.py\u001b[0m in \u001b[0;36m_stream_output\u001b[0;34m(process)\u001b[0m\n\u001b[1;32m    656\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexit_code\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 657\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Process exited with code: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mexit_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Process exited with code: 1",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-22d7d36998e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtf_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/sagemaker/tensorflow/estimator.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, inputs, wait, logs, job_name, run_tensorboard_locally)\u001b[0m\n\u001b[1;32m    441\u001b[0m                 \u001b[0mtensorboard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m             \u001b[0mfit_super\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/sagemaker/tensorflow/estimator.py\u001b[0m in \u001b[0;36mfit_super\u001b[0;34m()\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mfit_super\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensorFlow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_tensorboard_locally\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mwait\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, inputs, wait, logs, job_name)\u001b[0m\n\u001b[1;32m    337\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_for_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_TrainingJob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mstart_new\u001b[0;34m(cls, estimator, inputs)\u001b[0m\n\u001b[1;32m    861\u001b[0m         \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_spot_checkpoint_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 863\u001b[0;31m         \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtrain_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    864\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_current_job_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_mode, input_config, role, job_name, output_config, resource_config, vpc_config, hyperparameters, stop_condition, tags, metric_definitions, enable_network_isolation, image, algorithm_arn, encrypt_inter_container_traffic, train_use_spot_instances, checkpoint_s3_uri, checkpoint_local_path)\u001b[0m\n\u001b[1;32m    392\u001b[0m         \u001b[0mLOGGER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Creating training-job with name: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0mLOGGER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train request: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_request\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_training_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtrain_request\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m     def compile_model(\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/sagemaker/local/local_session.py\u001b[0m in \u001b[0;36mcreate_training_job\u001b[0;34m(self, TrainingJobName, AlgorithmSpecification, OutputDataConfig, ResourceConfig, InputDataConfig, **kwargs)\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0mtraining_job\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_LocalTrainingJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontainer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0mhyperparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"HyperParameters\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m\"HyperParameters\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m         \u001b[0mtraining_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mInputDataConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOutputDataConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhyperparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrainingJobName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0mLocalSagemakerClient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_training_jobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTrainingJobName\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_job\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/sagemaker/local/entities.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self, input_data_config, output_data_config, hyperparameters, job_name)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         self.model_artifacts = self.container.train(\n\u001b[0;32m---> 89\u001b[0;31m             \u001b[0minput_data_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_data_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhyperparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m         )\n\u001b[1;32m     91\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/sagemaker/local/image.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_data_config, output_data_config, hyperparameters, job_name)\u001b[0m\n\u001b[1;32m    151\u001b[0m             \u001b[0;31m# which contains the exit code and append the command line to it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Failed to run: %s, %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcompose_command\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0martifacts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve_artifacts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompose_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_data_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to run: ['docker-compose', '-f', '/tmp/tmpvs3dqwvv/docker-compose.yaml', 'up', '--build', '--abort-on-container-exit'], Process exited with code: 1"
     ]
    }
   ],
   "source": [
    "tf_estimator.fit(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# include the date in the endpoint \n",
    "tf_endpoint_name = 'keras-tf-fmnist-'+time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "\n",
    "# deploy the endpoint using GPU acceleration\n",
    "tf_predictor = tf_estimator.deploy(initial_instance_count=1,\n",
    "                         instance_type='ml.c5.large',\n",
    "                         accelerator_type='ml.eia1.medium',\n",
    "                         endpoint_name=tf_endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## perform predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " %matplotlib inline\n",
    "\n",
    "num_samples = 5\n",
    "indices = random.sample(range(x_val.shape[0] - 1), num_samples)\n",
    "images = x_val[indices]/255\n",
    "labels = y_val[indices]\n",
    "\n",
    "for i in range(num_samples):\n",
    "    plt.subplot(1,num_samples,i+1)\n",
    "    plt.imshow(images[i].reshape(28, 28), cmap='gray')\n",
    "    plt.title(labels[i])\n",
    "    plt.axis('off')\n",
    "    \n",
    "prediction = tf_predictor.predict(images.reshape(num_samples, 28, 28, 1))['predictions']\n",
    "prediction = np.array(prediction)\n",
    "predicted_label = prediction.argmax(axis=1)\n",
    "print('Predicted labels are: {}'.format(predicted_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameter_ranges = {\n",
    "    'epochs':        IntegerParameter(20, 100),\n",
    "    'learning-rate': ContinuousParameter(0.001, 0.1, scaling_type='Logarithmic'), \n",
    "    'batch-size':    IntegerParameter(32, 1024),\n",
    "    'dense-layer':   IntegerParameter(128, 1024),\n",
    "    'dropout':       ContinuousParameter(0.2, 0.6)\n",
    "}\n",
    "\n",
    "objective_metric_name = 'val_acc'\n",
    "objective_type = 'Maximize'\n",
    "metric_definitions = [{'Name': 'val_acc', 'Regex': 'val_acc: ([0-9\\\\.]+)'}]\n",
    "\n",
    "tuner = HyperparameterTuner(tf_estimator,\n",
    "                            objective_metric_name,\n",
    "                            hyperparameter_ranges,\n",
    "                            metric_definitions,\n",
    "                            max_jobs=10,\n",
    "                            max_parallel_jobs=2,\n",
    "                            objective_type=objective_type)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
